# 调整参数

## `待审核`1.问题：Fluid中如何随机初始化模型的权重？

+ 问题描述：在Fluid中是否提供相关的接口实现模型参数的随机初始化？

+ 相关代码：

```python
import paddle.fluid as fluid
fluid.default_startup_program()
```

+ 问题分析：
初始化权重其实就是在模型训练的一开始为模型结构中不同的节点先赋值，不同的赋值方式会造成不同的影响，随机初始化是常见的方式，即随机生成相关的权值赋予模型结构中的节点，其背后的意义就是破坏了模型的对称性，让训练更有效果。

+ 解决方法：

在使用default_startup_program()方法则可，该方法默认就会对全局进行随机初始化，即随机初始化参数是Fluid默认方式，你还可以通过传入随机种子的方式来控制随机初始化的随机性，如下：


```python
import paddle.fluid as fluid
fluid.default_startup_program().random_seed = 90
```


+ 问题拓展：
良好的初始化权重可以加快梯度下降的收敛速度，增加梯度下降收敛到较低训练（和泛化）错误的几率



## `待审核`2.问题：Fluid中如何实现自定义的权重初始化？

+ 问题描述：在Fluid中如何自定义的进行权重初始化？比如我想实现He initialization应该如何做？

+ 问题分析：
深度学习是快速发展的一个学科，Fluid难以实现其所有的权重初始化理论，但所有的权重初始化理论在工程操作上都是类似的，即以不同的算法来给模型中的节点初始化一个权值，所有在Fluid上实现自定初始化逻辑时比较简单的。

+ 解决方法：

以实现He initialization为例，代码如下：

```
import numpy as np

def he_init(layers_dims):
    np.random.seed(3)
    parameters = {}
    L = len(layers_dims) - 1  # 表示层数的整数

    for l in range(1, L + 1):
        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * np.sqrt(
            2. / layers_dims[l - 1])
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
    return parameters
```

+ 问题拓展：
He initialization在使用ReLU的模型结构中使用时非常适合的，这方面跟多内容可以参考：https://blog.liexing.me/2017/10/24/deep-learning-weight-initialization/


## `待审核`3.问题：测试模型与训练模型有较大的差异时，Fluid版的Paddle是否提供相关的方法支持这种情况？

+ 问题描述：Fluid定义好了训练模型，并使用`fluid.default_main_program().clone(for_test=True)`方法定义好了相应的测试模型，但现在需要的测试模型与训练模型是有比较大的差异的，如何处理这种情况？

+ 相关代码：

```python
img = fluid.layers.data(name="image", shape=[784])
prediction = fluid.layers.fc(
  input=fluid.layers.fc(input=img, size=100, act='relu'),
  size=10,
  act='softmax'
)
label = fluid.layers.data(name="label", shape=[1], dtype="int64")
loss = fluid.layers.mean(fluid.layers.cross_entropy(input=prediction, label=label))
acc = fluid.layers.accuracy(input=prediction, label=label)

test_program = fluid.default_main_program().clone(for_test=True)
```

+ 问题分析：

如果训练程序和测试程序相差较大时，用户也可以通过完全定义两个不同的 fluid.Program，分别进行训练和测试。

在PaddlePaddle Fluid中，所有的参数都有名字。如果两个不同的操作，甚至两个不同的网络使用了同样名字的参数，那么他们的值和内存空间都是共享的。

PaddlePaddle Fluid中使用 fluid.unique_name 包来随机初始化用户未定义的参数名称。通过 fluid.unique_name.guard 可以确保多次调用某函数参数初始化的名称一致。

+ 解决方法：

```
import paddle.fluid as fluid

def network(is_test):
    file_obj = fluid.layers.open_files(filenames=["test.recordio"] if is_test else ["train.recordio"], ...)
    img, label = fluid.layers.read_file(file_obj)
    hidden = fluid.layers.fc(input=img, size=100, act="relu")
    hidden = fluid.layers.batch_norm(input=hidden, is_test=is_test)
    ...
    return loss

with fluid.unique_name.guard():
    train_loss = network(is_test=False)
    sgd = fluid.optimizer.SGD(0.001)
    sgd.minimize(train_loss)

#测试网络
test_program = fluid.Program()
# 保证多次调用某函数参数初始化的名称一致
with fluid.unique_name.guard():
    with fluid.program_gurad(test_program, fluid.Program()):
        test_loss = network(is_test=True)
```

关于测试网络更多内容请参考：http://staging.paddlepaddle.org/documentation/docs/zh/0.14.0/new_docs/user_guides/howto/training/test_while_training.html


## `待审核`4.问题：Paddle在多GPU训练时下如何对参数进行初始化？

+ 问题描述：在使用Paddle进行多机训练时，Paddle是如何初始化不同GPU上的参数的？

+ 问题分析：
在单GPU下时，环境单一，参数随机初始化也就简单，通过如下方式调用则可：

```python
exe = fluid.Executor(fluid.CUDAPlace(0))
exe.run(program=fluid.default_startup_program())
```

但多GPU情况下，fluid.Executor()不可再使用，此时需要使用fluid.ParallelExecutor()方法来实现多GPU训练

+ 解决方法：

使用多GPU训练，参数需要先在GPU0上初始化，再经由fluid.ParallelExecutor()分发到多张显卡上


















