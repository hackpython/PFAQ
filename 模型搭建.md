# 模型搭建


## `已审核`1.问题：如何使用Fluid的流程控制？

+ 问题描述：参考了官方文档中主要的几个例子，似乎都是常见的模式，如何使用Fluid的流程机制？

+ 问题解答：

流程控制方面的内容都放在了`fluid.layers.control_flow`模块下，里面包含了While、Block、Conditional、Switch、if、ifelse等跟中操作，这样可以让在编写模型时，像编写普通的python程序一样。

```
lr = fluid.layers.tensor.create_global_var(
    shape=[1],
    value=0.0,
    dtype='float32',
    persistable=True,
    name='learning_rate'
)
one_var = fluid.layers.fill_constant(
    shape=[1], dtype='float32', value=1.0
)
two_var = fluid.layers.fill_constant(
    shape=[1], dtype='float32', value=2.0
)

# switch
with fluid.layers.control_flow.Switch() as switch:
    with switch.case(global_step == one_var):
        fluid.layers.tensor.assign(input=one_var, output=lr)
        with switch.default():
            fluid.layers.tensor.assign(input=two_var, output=lr)
```


+ 问题拓展：

动态计算意味着程序将按照我们编写命令的顺序进行执行。这种机制将使得调试更加容易，并且也使得我们将大脑中的想法转化为实际代码变得更加容易。而静态计算则意味着程序在编译执行时将先生成神经网络的结构，然后再执行相应操作。从理论上讲，静态计算这样的机制允许编译器进行更大程度的优化，但是这也意味着你所期望的程序与编译器实际执行之间存在着更多的代沟。这也意味着，代码中的错误将更加难以发现（比如，如果计算图的结构出现问题，你可能只有在代码执行到相应操作的时候才能发现它）。




## `已审核`2.问题：如何获得模型的参数？

+ 问题描述：在训练模型时，如何获得模型的参数？

+ 问题分析：在Fluid中要获得模型对应的参数，需要理解Fluid中Program方面的设计，主要就是要理解Programs and Blocks，在Fluid 的 Program 的基本结构是一些嵌套 blocks，形式上类似一段 C++ 或 Java 程序。

blocks中包含：
1.本地变量的定义
2.一系列的operator

那么通过Program的blocks就可以获得对应网络的参数节点对象


+ 解决方法：

一段示例代码如下：

```
program = fluid.Program()

p.name for p in program.global_block().all_parameters()
```

这里通过fluid.Program()定义program实例，然后通过global_block().all_parameters()获得该program所有节点的对象。

更多相关内容可以参考Fluid设计方面的文档：http://paddlepaddle.org/documentation/docs/zh/1.1/user_guides/design_idea/fluid_design_idea.html

## `待审核`3.问题：在构建LSTM模型时，使用了Fluid的序列类型，此时我应该如何将本地值传递给序列类似？

+ 问题描述：在构建LSTM模型时，使用了Fluid的序列类型，此时我应该如何将本地值传递给序列类似？

+ 问题分析：Fliud使用特有的LoDTensor数据类型来支持序列数据，使用 LoDTensor 作为 输入数据类型需要用户传入一个mini-batch需要被训练的所有数据与每个序列的长度信息。 用户可以使用 fluid.create_lod_tensor 来创建 LoDTensor。

+ 解决方法：

要传入序列数据，形式如下：

```python
test = fluid.layers.data(name="test", dtype="float32", shape=[1], lod_level=1)


exe.run(feed={
  "test": create_lod_tensor(
    data=numpy.array([1, 3, 4, 5, 3, 6, 8], dtype='float32').reshape(-1, 1),
    lod=[4, 1, 2],
    place=fluid.CPUPlace()
  )
})
```

传入序列信息的时候，需要设置序列嵌套深度，lod_level。 例如训练数据是词汇组成的句子，lod_level=1；训练数据是 词汇先组成了句子， 句子再组成了段落，那么 lod_level=2。

在上述的实例代码中，训练数据 test 包含三个样本，他们的长度分别是 4, 1, 2。 他们分别是 data[0:4]， data[4:5] 和 data[5:7]。


## `待审核`4.问题：paddlepaddle Fluid是否有参数绑定机制

+ 问题描述：想实现多个卷积层使用同一个卷积核，了解到需要param_name一致，但没有从中找到对应的方法。

+ 问题分析：
在使用Fluid编写模型的过程中，通常会给不同的节点使用不同的名称，要使用不同的节点时，可以通过相应的节点名称找到节点本身，可以认为节点名称就是模型在训练时对应的变量名，在训练过程中可以通过这个变量名来找到该节点，该问题可以在不同的卷积层使用同名的卷积核则可。

+ 解决方法：

在使用conv2d()方法构建卷积层时，设定相同的name参数则可。

```python
def conv2d(input,
           num_filters,
           filter_size,
           stride=1,
           padding=0,
           dilation=1,
           groups=None,
           param_attr=None,
           bias_attr=None,
           use_cudnn=True,
           act=None,
           name=None):
```

## `待审核` 5.问题：`test_prog = test_prog.clone(for_test=True)`具有什么意义？

+ 问题描述：不太理解`test_prog = test_prog.clone(for_test=True)`的含义，我在快速入门中看到的都是克隆main_program，不是克隆自己，再赋值给自己

+ 相关代码：

```python
def train(args):
    # parameters from arguments
    model_name = args.model
    checkpoint = args.checkpoint
    pretrained_model = args.pretrained_model
    with_memory_optimization = args.with_mem_opt
    model_save_dir = args.model_save_dir

    startup_prog = fluid.Program()
    train_prog = fluid.Program()
    test_prog = fluid.Program()
    if args.enable_ce:
        startup_prog.random_seed = 1000
        train_prog.random_seed = 1000

    train_py_reader, train_cost, train_acc1, train_acc5 = build_program(
        is_train=True,
        main_prog=train_prog,
        startup_prog=startup_prog,
        args=args)
    test_py_reader, test_cost, test_acc1, test_acc5 = build_program(
        is_train=False,
        main_prog=test_prog,
        startup_prog=startup_prog,
        args=args)
    test_prog = test_prog.clone(for_test=True)
```

代码来源：https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/image_classification/train.py


+ 问题分析：
Fluid中clone()方法用于创建一个新的与调用网络相同的program

但有些operator，在训练和测试之间的行为是不同的，比如batch_norm。它们有一个属性is_test来控制行为。当for_test=True时，此方法将把它们的is_test属性更改为True。

+ 克隆Program，该Program用于训练时，将 for_test 设置为False。
+ 克隆Program，该Program用于测试时，将 for_test 设置为True。

+ 问题解答：

看到来源代码中的build_program方法，代码如下：

```python
def build_program(is_train, main_prog, startup_prog, args):
    image_shape = [int(m) for m in args.image_shape.split(",")]
    model_name = args.model
    model_list = [m for m in dir(models) if "__" not in m]
    assert model_name in model_list, "{} is not in lists: {}".format(args.model,
                                                                     model_list)
    model = models.__dict__[model_name]()
    with fluid.program_guard(main_prog, startup_prog):
        py_reader = fluid.layers.py_reader(
            capacity=16,
            shapes=[[-1] + image_shape, [-1, 1]],
            lod_levels=[0, 0],
            dtypes=["float32", "int64"],
            use_double_buffer=True)
        with fluid.unique_name.guard():
            image, label = fluid.layers.read_file(py_reader)
            avg_cost, acc_top1, acc_top5 = net_config(image, label, model, args)
            avg_cost.persistable = True
            acc_top1.persistable = True
            acc_top5.persistable = True
            if is_train:
                params = model.params
                params["total_images"] = args.total_images
                params["lr"] = args.lr
                params["num_epochs"] = args.num_epochs
                params["learning_strategy"]["batch_size"] = args.batch_size
                params["learning_strategy"]["name"] = args.lr_strategy

                optimizer = optimizer_setting(params)
                optimizer.minimize(avg_cost)

    return py_reader, avg_cost, acc_top1, acc_top5
```

build_program里的is_train=False，并没有将program_desc里op的is_test属性改成false。它只是不加一些训练需要的参数。因此，必须使用clone(for_test=True)来将ProgramDesc里op的is_test属性改成false。



























