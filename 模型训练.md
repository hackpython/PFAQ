# 模型训练

## `待审核`1.问题：Fluid接口是否可以同时训练2个不同的网络

+ 问题描述：我是否可以使用Fluid版本的PaddlePaddle实现同时训练2个不同的网络

+ 问题解答：
这是可以的，同时训练2个不同的网络是比较常见的需求，Fluid可以请求实现这个需求，简单而言你只需要定义两个网络结构，然后调用执行器分别执行这两个网络这可。

+ 解决方法：

这里定义简单的GAN来实现同时训练2个不同的网络，部分代码如下：

```
def D(x):
    hidden = fluid.layers.fc(input=x,
                             size=200,
                             act='relu',
                             param_attr='D.w1',
                             bias_attr='D.b1')
    logits = fluid.layers.fc(input=hidden,
                             size=1,
                             act=None,
                             param_attr='D.w2',
                             bias_attr='D.b2')
    return logits


def G(x):
    hidden = fluid.layers.fc(input=x,
                             size=200,
                             act='relu',
                             param_attr='G.w1',
                             bias_attr='G.b1')
    img = fluid.layers.fc(input=hidden,
                          size=28 * 28,
                          act='tanh',
                          param_attr='G.w2',
                          bias_attr='G.b2')
    return img


def main():
	 ...

	 with fluid.program_guard(dg_program, startup_program):
	    noise = fluid.layers.data(
	        name='noise', shape=[NOISE_SIZE], dtype='float32')
	    g_img = G(x=noise)
	    g_program = dg_program.clone()
	    dg_loss = fluid.layers.sigmoid_cross_entropy_with_logits(
	        x=D(g_img),
	        label=fluid.layers.fill_constant_batch_size_like(
	            input=noise, dtype='float32', shape=[-1, 1], value=1.0))
	    dg_loss = fluid.layers.mean(dg_loss)


	 ...

	 generated_img = exe.run(g_program,
                                    feed={'noise': n},
                                    fetch_list={g_img})[0]

      real_data = numpy.array([x[0] for x in data]).astype('float32')
            real_data = real_data.reshape(num_true, 784)
            total_data = numpy.concatenate([real_data, generated_img])
            total_label = numpy.concatenate([
                numpy.ones(
                    shape=[real_data.shape[0], 1], dtype='float32'),
                numpy.zeros(
                    shape=[real_data.shape[0], 1], dtype='float32')
            ])
            d_loss_np = exe.run(d_program,
                                feed={'img': total_data,
                                      'label': total_label},
                                fetch_list={d_loss})[0]                              
```

完整代码请参考：https://github.com/PaddlePaddle/Paddle/blob/6c80bb3ce964f1b6271ffad1d9614e93e0882c1d/python/paddle/fluid/tests/demo/fc_gan.py

## `待审核`2.问题：Fluid可以在模型间共享参数吗

+ 问题描述：Fluid版本的PaddlePaddle似乎没有提供接口实现模型间的参数共享，请问Fluid是否可以实现这种功能？

+ 问题分析：所谓模型间共享参数，其实就是不同的节点间使用相同的值，来实现权重共享，一个简单的方法就是不同的节点使用相同的节点名，这样在训练时，同名节点其参数其实就共享了，这种tick还是比较常见的。

+ 解决方法：

将相应layer的参数名设置成相同的名称，实现模型结构件的参数共享

```python
import paddle.fluid as fluid

fc1 = fluid.layers.fc(
    input=input1,
    param_attr=fluid.param_attr.ParamAttr(name='fc.w'),
    bias_attr=fluid.param_attr.ParamAttr(name='fc.b'))

fc2 = fluid.layers.fc(
    input=input2,
    param_attr=fluid.param_attr.ParamAttr(name='fc.w'),
    bias_attr=fluid.param_attr.ParamAttr(name='fc.b'))
```
