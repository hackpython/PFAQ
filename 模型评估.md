# 模型评估

## `待审核`1.问题：使用Fluid时，为何要分别实现训练模型与预测模型

+ 问题描述：在使用Fluid构建模型时有个疑惑，如线性回归等模型中，为何要定义一个与训练网络几乎一样的网络来做预测？

+ 相关代码：

```python
def train_program():
    y = fluid.layers.data(name='y', shape=[1], dtype='float64')
    x = fluid.layers.data(name='x', shape=[13], dtype='float64')

    y_predict = fluid.layers.fc(input=x, size=1, act=None)
    # 平均损失
    loss = fluid.layers.square_error_cost(input=y_predict, label=y)
    avg_loss = fluid.layers.mean(loss)
    return avg_loss


def inference_program():
    mm = fluid.layers.data(name='mm', shape=[13], dtype='float64')
    y_predict = fluid.layers.fc(input=mm, size=1, act=None)
    return y_predict
```

+ 问题分析：
训练模型与预测模型在结构上类似，可以直接使用clone()方法来拷贝一份结构，整体逻辑就是先加载此前训练好的模型，然后再将加载的模型拷贝一份。

+ 解决方法：

要简化上面的代码，可以使用如下形式的代码，使用fluid.io.load_inference_model()方法来加载网络和经过训练的参数，然后通过clone()方法将加载的模型拷贝用于预测。

```python
place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
exe = fluid.Executor(place)
inference_scope = fluid.core.Scope()

with fluid.scope_guard(inference_scope):

    [inference_program, feed_target_names,
     fetch_targets] = fluid.io.load_inference_model(params_dirname, exe)

        # The input's dimension of conv should be 4-D or 5-D.
        # Use inference_transpiler to speedup
    inference_transpiler_program = inference_program.clone()
    t = fluid.transpiler.InferenceTranspiler()
    t.transpile(inference_transpiler_program, place)

        # Construct feed as a dictionary of {feed_target_name: feed_target_data}
        # and results will contain a list of data corresponding to fetch_targets.
    results = exe.run(inference_program,
                      feed={feed_target_names[0]: img},
                      fetch_list=fetch_targets)

    transpiler_results = exe.run(inference_transpiler_program,
                                 feed={feed_target_names[0]: img},
                                 fetch_list=fetch_targets)
```

更多细节可以参考：http://www.paddlepaddle.org/documentation/docs/zh/1.2/beginners_guide/basics/image_classification/index.html#permalink-12-infererence-program-


## `待审核`2.问题：在Fluid中如何生成测试网络对模型进行测试？

+ 问题描述：使用Fluid编写好训练完模型后，如何定义测试网络对已有模型进行测试？是否要自己重新编写一个test模型？

+ 相关代码：

```python
import paddle.fluid as fluid

def train_program():
    img = fluid.layers.data(name='img', shape=[28,28])
    prediction = fluid.layers.fc(
        input=fluid.layers.fc(input=img, size=100, act='relu'),
        size=10,
        act='softmax'
    )
    label = fluid.layers.data(name="label", shape=[1], dtype="int64")
    loss = fluid.layers.mean(fluid.layers.cross_entropy(input=prediction, label=label))
    acc = fluid.layers.accuracy(input=prediction, label=label)
    adam = fluid.optimizer.Adam(learning_rate=0.001)
    adam.minimize(loss)
```


+ 问题分析：
测试模型与训练模型在使用时是不相同的，比如测试时通常不使用BatchNorm层，Fluid为了简化测试模型的使用提供了相应的操作方法，可以实现快速生成测试模型

+ 解决方法：

使用fluid.default_main_program().clone(for_test=True)来获得测试模型

```python
import paddle.fluid as fluid

def train_program():
    img = fluid.layers.data(name='img', shape=[28,28])
    prediction = fluid.layers.fc(
        input=fluid.layers.fc(input=img, size=100, act='relu'),
        size=10,
        act='softmax'
    )
    label = fluid.layers.data(name="label", shape=[1], dtype="int64")
    loss = fluid.layers.mean(fluid.layers.cross_entropy(input=prediction, label=label))
    test_program = fluid.default_main_program().clone(for_test=True)

    acc = fluid.layers.accuracy(input=prediction, label=label)
    adam = fluid.optimizer.Adam(learning_rate=0.001)
    adam.minimize(loss)
```

在使用 Optimizer 之前，将 fluid.default_main_program() 复制成一个 test_program 。之后使用测试数据运行 test_program,就可以做到运行测试程序，而不影响训练结果。


## `待审核`3.问题：Fluid如何自定义评估指标？

+ 问题描述：在Fluid中如何自定义评估指派？而不是单纯的使用Fluid已提供的评估指标？

+ 问题分析：

目前Fluid已经提供多种指标分别用于评估不同的模型，如分类任务评价指标有Precision、Accuracy、Recall、Auc等等，当然Fluid也支持定义评估指标以支持各类任务。

+ 解决方法：

定义一个简单的计数器metric函数实现对模型的评估，代码中preds是模型的预测值，而labels是真实的标签值，实例程序为：

```python
from paddle.fluid.metrics import MetricBase

class TestMetric(MetricBase):
    def __init__(self, name=None):
        super(TestMetric, self).__init__(name)
        self.counter = 0  # simple counter

    def reset(self):
        self.counter = 0

    def update(self, preds, labels):
        if not _is_numpy_(preds):
            raise ValueError("The 'preds' must be a numpy ndarray.")
        if not _is_numpy_(labels):
            raise ValueError("The 'labels' must be a numpy ndarray.")
        self.counter += sum(preds == labels)

    def eval(self):
        return self.counter
```


## `待审核`4.问题：如何使用Paddle来衡量NLP模型生成的句子是否具有意义？

+ 问题描述：使用Fluid版的Paddle构建了一个NLP模型后，如何才能衡量生成语句的意义？

+ 问题分析：
要判断NLP模型生成内容是否具有意义通常有两种做法，一种是才有多分类的方式进行评估，另一种即使用编辑距离的方式，Fluid中提供EditDistance()方法来衡量两个字符串的相似度。

+ 解决方法：

使用EditDistance()方法来衡量NLP模型输出结构的相似度，从而判断生成的句子是否有意义，实例代码如下：

```python
import paddle.fluid as fluid

distances, seq_num = fluid.layers.edit_distance(input, label)
distance_evaluator = fluid.metrics.EditDistance()
for epoch in PASS_NUM:
    distance_evaluator.reset()
    for data in batches:
        loss = exe.run(fetch_list=[cost] + list(edit_distance_metrics))
    distance_evaluator.update(distances, seq_num)
    distance, instance_error = distance_evaluator.eval()
```

+ 问题拓展：

编辑距离是通过计算将一个字符串转换为另一个字符串所需的最小操作数量来量化两个字符串（例如，字）彼此不相似的方式。

关于编辑距离的更多内容，请参考相关的wiki：https://en.wikipedia.org/wiki/Edit_distance




















